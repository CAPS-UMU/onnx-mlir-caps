// Filename: FusedGemmRuntime_omtensor.cpp
/* Description:
 * This file implements a custom FusedGemm runtime function for ONNX-MLIR.
 * It provides a C++ implementation of a fused Gemm (matrix multiplication + bias + ReLU)
 * using the OMTensor API, supporting bias broadcasting and optional activation.
 */

/**********************************************
 * IMPORT LIBRARIES
 **********************************************/
#include <cstdint>
#include <cmath>
#include <cstdio>
#include <cstring>
#include <algorithm>
#include <vector>
#include <string>

// ONNX-MLIR Runtime API
#include "OnnxMlirRuntime.h"

// ONNX Runtime C++ API (latest as of cutoff: v1.16.1)
#include "onnxruntime_cxx_api.h"

// Include the raw .onnx model bytes (generated by xxd from gemm.onnx)
// #include "gemm_relu_model.inc"
#include FUSED_INCL_FILENAME

/**********************************************
 * HELPER FUNCTION DEFINITIONS
 **********************************************/
#include <numeric> // For std::accumulate

/**
 * @brief Computes the product of the dimensions of a tensor.
 *
 * @param tensor Pointer to an OMTensor.
 * @return size_t The number of elements in the tensor.
 */
static size_t numElements(OMTensor* tensor) {
  size_t rank = omTensorGetRank(tensor);
  const int64_t* shape = omTensorGetShape(tensor);
  size_t prod = 1;
  for (size_t i = 0; i < rank; ++i) {
    prod *= shape[i];
  }
  return prod;
}

/**********************************************
 * MAIN RUNTIME FUNCTION DEFINITION
 **********************************************/
/**
 * @brief Executes the fused Gemm+ReLU operation via an in-memory ONNX model.
 *
 * Initializes an ONNX Runtime session with the embedded gemm_relu.onnx model,
 * creates input tensors using the data from the provided OMTensors,
 * runs the model, and writes the output data back into Y_omTensor.
 *
 * @param Y_omTensor Pointer to the output OMTensor.
 * @param A_omTensor Pointer to input tensor A.
 * @param B_omTensor Pointer to input tensor B.
 * @param Bias_omTensor Pointer to input tensor Bias (Gemm's C input).
 * @param activation Unused in this example.
 * @param alpha Scaling factor for Gemm.
 * @param beta Scaling factor for bias.
 * @param domain_name Unused in this example.
 * @param onnx_node_name Unused in this example.
 * @param transA Transpose flag for A.
 * @param transB Transpose flag for B.
 */
extern "C" void FusedGemm(
    OMTensor* Y_omTensor,
    OMTensor* A_omTensor,
    OMTensor* B_omTensor,
    OMTensor* Bias_omTensor, // Corresponds to Gemm's 'C' input
    const char *activation,
    float alpha,
    float beta,
    const char *domain_name,
    const char *onnx_node_name,
    int64_t transA,
    int64_t transB
) {
    /**********************************************
     * VALIDATE INPUT TENSORS
     **********************************************/
    if (!A_omTensor || !B_omTensor || !Y_omTensor) {
        fprintf(stdout, "Error: One or more input tensors are null.\n");
        return;
    }
    if (omTensorGetDataType(A_omTensor) != ONNX_TYPE_FLOAT ||
        omTensorGetDataType(B_omTensor) != ONNX_TYPE_FLOAT ||
        omTensorGetDataType(Y_omTensor) != ONNX_TYPE_FLOAT) {
        fprintf(stdout, "Error: Input tensors must be of type float.\n");
        return;
    }
    
    /**********************************************
     * RETRIEVE TENSOR INFO FROM OMTensors
     **********************************************/
    const int64_t* A_shape = omTensorGetShape(A_omTensor);
    const int64_t* B_shape = omTensorGetShape(B_omTensor);
    const int64_t* Bias_shape = Bias_omTensor ? omTensorGetShape(Bias_omTensor) : nullptr;
    const int64_t* Y_shape = omTensorGetShape(Y_omTensor);
    
    int64_t M = Y_shape[0];
    int64_t N = Y_shape[1];
    
    float* A_ptr = static_cast<float*>(omTensorGetDataPtr(A_omTensor));
    float* B_ptr = static_cast<float*>(omTensorGetDataPtr(B_omTensor));
    float* Bias_ptr = Bias_omTensor ? static_cast<float*>(omTensorGetDataPtr(Bias_omTensor)) : nullptr;
    float* Y_ptr = static_cast<float*>(omTensorGetDataPtr(Y_omTensor));
    
    /**********************************************
     * SET UP ONNX RUNTIME ENVIRONMENT
     **********************************************/
    try {
        Ort::Env env(ORT_LOGGING_LEVEL_WARNING, "fused_gemm_env");
        Ort::SessionOptions session_options;
        session_options.SetIntraOpNumThreads(1);
        
        // Create a session with the in-memory model data.
        Ort::Session session(env,
            reinterpret_cast<const void*>(gemm_relu_onnx),
            static_cast<size_t>(gemm_relu_onnx_len),
            session_options);
        
        Ort::MemoryInfo memory_info = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);
        
        /**********************************************
         * CREATE INPUT TENSORS
         **********************************************/
        // Compute the number of elements from tensor shapes.
        size_t A_numel = numElements(A_omTensor);
        size_t B_numel = numElements(B_omTensor);
        size_t C_numel = Bias_omTensor ? numElements(Bias_omTensor) : 0;
        
        std::vector<int64_t> dims_A(A_shape, A_shape + omTensorGetRank(A_omTensor));
        std::vector<int64_t> dims_B(B_shape, B_shape + omTensorGetRank(B_omTensor));
        std::vector<int64_t> dims_C;
        if (Bias_omTensor) {
            dims_C.assign(Bias_shape, Bias_shape + omTensorGetRank(Bias_omTensor));
        }
        
        Ort::Value input_A = Ort::Value::CreateTensor<float>(
            memory_info, A_ptr, A_numel, dims_A.data(), dims_A.size());
        Ort::Value input_B = Ort::Value::CreateTensor<float>(
            memory_info, B_ptr, B_numel, dims_B.data(), dims_B.size());
        Ort::Value input_C = Ort::Value::CreateTensor<float>(
            memory_info, Bias_ptr, C_numel, dims_C.data(), dims_C.size());
        
        const char* input_node_names[] = {"A", "B", "C"};
        Ort::Value input_tensors[] = { std::move(input_A), std::move(input_B), std::move(input_C) };
        
        /**********************************************
         * RUN THE MODEL
         **********************************************/
        // The model output is assumed to be named "Z" (result of the Relu node).
        const char* output_node_names[] = {"Z"};
        auto output_tensors = session.Run(
            Ort::RunOptions{nullptr},
            input_node_names, input_tensors, 3,
            output_node_names, 1);
        
        float* output_data = output_tensors[0].GetTensorMutableData<float>();
        size_t output_size = output_tensors[0].GetTensorTypeAndShapeInfo().GetElementCount();
        
        /**********************************************
         * WRITE BACK THE OUTPUT
         **********************************************/
        std::memcpy(Y_ptr, output_data, output_size * sizeof(float));
        
        fprintf(stdout, "FusedGemm: Successfully ran gemm_relu.onnx model; output tensor updated.\n");
    } catch (const Ort::Exception& e) {
        fprintf(stdout, "FusedGemm: ONNX Runtime exception: %s\n", e.what());
    }
}