//////////////////////////////////////////////
// IMPORT LIBRARIES //////////////////////////
//////////////////////////////////////////////
/**
 * @file only_gemm_inmem_onnx_model.cpp
 * @brief Demonstrates running an in-memory ONNX GEMM model using ONNX Runtime C++ API.
 *
 * This program initializes ONNX Runtime, loads an ONNX model (GEMM operation)
 * from an in-memory byte array (`gemm_model.inc`), prepares input tensors,
 * executes the model, and prints the output tensor.
 *
 * Libraries and packages used in this script and Versions Info:
 * - `<iostream>`: Standard C++ Library for input/output operations.
 * - `<vector>`: Standard C++ Library for dynamic arrays.
 * - `<cassert>`: Standard C++ Library for assertions.
 * - `<onnxruntime_cxx_api.h>`: ONNX Runtime C++ API. Version: 1.22.0 (as specified in project build script `c++_compile_script.sh`)
 * - `"gemm_model.inc"`: Contains the ONNX model as a byte array (generated by `xxd`).
 */
#include <cassert>
#include <iostream>
#include <vector>
#include <onnxruntime_cxx_api.h> // Version: 1.22.0 (as specified in project build script c++_compile_script.sh)

// Include the raw .onnx model bytes (generated by xxd from gemm.onnx)
#include "gemm_model.inc"

///////////////////////////////////////////////
// CONSTANTS & PARAMETERS /////////////////////
///////////////////////////////////////////////
/**
 * @brief Constants and parameters used in this script.
 */
// Define dimensions for the input and output tensors of the Gemm operation
static const std::vector<int64_t> DIMS_A{2, 3}; ///< Dimensions for tensor A (input matrix)
static const std::vector<int64_t> DIMS_B{3, 2}; ///< Dimensions for tensor B (input matrix)
static const std::vector<int64_t> DIMS_C{2, 2}; ///< Dimensions for tensor C (input bias matrix) and Y (output matrix)

//////////////////////////////////////////////
// FUNCTION DEFINITIONS //////////////////////
//////////////////////////////////////////////

/**
 * @brief Runs a Gemm operation (Y = A*B + C) using an in-memory ONNX model.
 *
 * Initializes an ONNX Runtime session with the embedded ONNX model data.
 * Creates input tensors for A, B, and C, executes the session,
 * and returns the resulting tensor Y.
 *
 * @param A_data A constant reference to a vector of floats representing tensor A.
 * @param B_data A constant reference to a vector of floats representing tensor B.
 * @param C_data A constant reference to a vector of floats representing tensor C (bias).
 * @return std::vector<float> The resulting tensor Y as a vector of floats.
 */
std::vector<float> runGemm(const std::vector<float>& A_data,
                           const std::vector<float>& B_data,
                           const std::vector<float>& C_data) {
  // Initialize ONNX Runtime environment
  Ort::Env env(ORT_LOGGING_LEVEL_WARNING, "gemm_example_env");

  // Initialize session options
  Ort::SessionOptions session_options;
  session_options.SetIntraOpNumThreads(1); // Example: configure for single-threaded execution

  // Create a session with the in-memory model data
  Ort::Session session(env,
      reinterpret_cast<const void*>(gemm_onnx), // Pointer to model data from gemm_model.inc
      static_cast<size_t>(gemm_onnx_len),       // Length of model data from gemm_model.inc
      session_options);

  // Get memory information for CPU
  Ort::MemoryInfo memory_info =
      Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);

  // Create input tensors from the provided data
  // Note: const_cast is used because CreateTensor expects non-const data*,
  // but ONNX Runtime will not modify input tensor data.
  Ort::Value input_A = Ort::Value::CreateTensor<float>(
      memory_info, const_cast<float*>(A_data.data()), A_data.size(), DIMS_A.data(), DIMS_A.size());
  Ort::Value input_B = Ort::Value::CreateTensor<float>(
      memory_info, const_cast<float*>(B_data.data()), B_data.size(), DIMS_B.data(), DIMS_B.size());
  Ort::Value input_C = Ort::Value::CreateTensor<float>(
      memory_info, const_cast<float*>(C_data.data()), C_data.size(), DIMS_C.data(), DIMS_C.size());

  // Define input tensor names (must match the names in the ONNX model)
  const char* input_node_names[] = {"A", "B", "C"};
  Ort::Value input_tensors[] = {std::move(input_A), std::move(input_B), std::move(input_C)};

  // Define output tensor names (must match the names in the ONNX model)
  const char* output_node_names[] = {"Y"};

  // Run the inference
  auto output_tensors = session.Run(
      Ort::RunOptions{nullptr}, input_node_names, input_tensors, 3, // Number of inputs
      output_node_names, 1);                                       // Number of outputs

  // Get pointer to the output data
  float* output_Y_ptr = output_tensors[0].GetTensorMutableData<float>();
  // Get the size of the output tensor
  size_t output_Y_size = output_tensors[0].GetTensorTypeAndShapeInfo().GetElementCount();

  // Copy output data to a std::vector and return it
  return std::vector<float>(output_Y_ptr, output_Y_ptr + output_Y_size);
}

//////////////////////////////////////////////////////////////////////////////////
// MAIN PROGRAM //////////////////////////////////////////////////////////////////
//////////////////////////////////////////////////////////////////////////////////
/**
 * @brief Main entry point of the program.
 *
 * Initializes sample input data, calls the `runGemm` function to perform
 * the Gemm operation, and then prints the resulting output tensor.
 */
int main() {
  /////////////////////////////////////////
  // INITIALIZE INPUT DATA //////////////
  /////////////////////////////////////////
  // Define hard-coded input data for tensors A, B, and C
  const std::vector<float> A_input = {1.0f, 2.0f, 3.0f, 4.0f, 5.0f, 6.0f}; // Shape {2, 3}
  const std::vector<float> B_input = {7.0f, 8.0f, 9.0f, 10.0f, 11.0f, 12.0f}; // Shape {3, 2}
  const std::vector<float> C_input = {1.0f, 1.0f, 1.0f, 1.0f}; // Shape {2, 2}

  std::cout << "Input A: { ";
  for(float val : A_input) std::cout << val << " ";
  std::cout << "}" << std::endl;

  std::cout << "Input B: { ";
  for(float val : B_input) std::cout << val << " ";
  std::cout << "}" << std::endl;

  std::cout << "Input C: { ";
  for(float val : C_input) std::cout << val << " ";
  std::cout << "}" << std::endl;


  /////////////////////////////////////////
  // EXECUTE GEMM OPERATION ///////////////
  /////////////////////////////////////////
  // Call the runGemm function with the input data
  std::vector<float> Y_output = runGemm(A_input, B_input, C_input);

  /////////////////////////////////////////
  // OUTPUT RESULTS ///////////////////////
  /////////////////////////////////////////
  // Print the dimensions and values of the output tensor Y
  std::cout << "Output Y (dims: [" << DIMS_C[0] << "," << DIMS_C[1] << "]): { ";
  for (size_t i = 0; i < Y_output.size(); ++i) {
    std::cout << Y_output[i] << (i == Y_output.size() - 1 ? "" : " ");
  }
  std::cout << " }" << std::endl;

  // Expected output for the given inputs:
  // A = [[1, 2, 3], [4, 5, 6]]
  // B = [[7, 8], [9, 10], [11, 12]]
  // C = [[1, 1], [1, 1]]
  // A*B = [[1*7+2*9+3*11, 1*8+2*10+3*12], [4*7+5*9+6*11, 4*8+5*10+6*12]]
  //     = [[7+18+33, 8+20+36], [28+45+66, 32+50+72]]
  //     = [[58, 64], [139, 154]]
  // Y = A*B + C = [[58+1, 64+1], [139+1, 154+1]]
  //             = [[59, 65], [140, 155]]
  // So, Y_output should be {59, 65, 140, 155}

  return 0;
}